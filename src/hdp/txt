http://stats.stackexchange.com/questions/135736/hierarchical-dirichlet-processes-in-topic-modeling

Hierarchical Dirichlet Processes in topic modeling
up vote
0
down vote
favorite
3
    

I think I understand the main ideas of hierarchical dirichlet processes, but I don't understand the specifics of its application in topic modeling. Basically, the idea is that we have the following model:

G0∼DP(γ,H)
Gj∼DP(α0,G0)
ϕji∼Gj
xji∼F(ϕji)

We sample from a Dirichlet process with a base distribution H to obtain a discrete distribution G0. Then, we use G0 in another Dirichlet process Gj for every j (in topic modeling, j is supposed to represent documents and Gj is a distribution over topics for document j). After this, for each word in document j, sample from Gj in order to select a particular topic. Some sources say that this is parameter associated to the topic and not properly a topic. In any case, this is acting as a latent variable. Finally, for each document j and word i, xji is described as a distribution F that depends on the latent variable ϕji associated in some way to the selected topic.

The question is: How do you describe explicitly F(ϕji)? I think I have seen a multinomial distribution there, but I'm not sure about it. As a comparison, in LDA we need for each topic a distribution over words and a multinomial distribution is required. What is the equivalent procedure here and what it represents in terms of words, documents and topics?
clustering dirichlet-distribution topic-models
shareedit
    
edited Jan 31 at 2:51

    
asked Jan 31 at 0:35
Robert Smith
946520
    
     
    
I don't have a direct answer to your question, but just wanted to share a related and maybe even relevant recent paper, which I ran across just today. It contains a brief overview of topic modeling and classification, PLSA and LDA approaches, as well as a novel LDA algorithm for high reproducibility, accuracy and computational efficiency. Hope it helps. –  Aleksandr Blekh Jan 31 at 0:58
     
    
It looks interesting. Thanks for sharing. –  Robert Smith Jan 31 at 1:02
     
    
You're welcome. This paper might be much closer to what you're looking for. –  Aleksandr Blekh Jan 31 at 1:20
     
    
Sure, but I think that nested model is a more elaborated version of the model I'm referring to. –  Robert Smith Jan 31 at 2:52
     
    
That's true. I was just hoping that you could infer some useful information from the commonality between the two versions (description, math). I'm sorry that I can't be of more help with that. –  Aleksandr Blekh Jan 31 at 3:09
show 1 more comment
1 Answer
active
oldest
votes
up vote
0
down vote
    

I found this truly excellent review that describes precisely how Hierarchical Dirichlet Processes work. [1]

First, start by choosing a base distribution H. In the case of topic modeling, we have a Dirichlet distribution as a prior for H. The dimension of this distribution should describe a distribution of words for each topic. Therefore, the dimension should be equal to the size of the vocabulary V. In the example described in the review, the author assumes a vocabulary of 10 words, so he uses H=Dirichlet(1/10,...,1/10). As usual, a realization of this distribution generates a 10-dimensional vector θk of proportions.

After this, H is used to build a Dirichlet Process DP(γ,H) and a realization G0 of this process is another discrete distribution with locations {θk} where each θk describes the distribution over words for a topic k. If we use G0 as a base distribution for another Dirichlet Process DP(α0,G0), it is possible to obtain a realization Gj for every document j in such a way that Gj has the same support as G0. Therefore, every Gj shares the same set of θk's, although with different proportions (which are called mixing weights in the definition of a Dirichlet Process)

Finally, for every document j and every word i, we draw a realization from Gj which generates a particular vector θk. Since this θk is a distribution over words for a given topic, we only need to sample from a multinomial distribution using θk as parameter in order to sample words wji=Multinomial(θk).

I have seen that sometimes ϕji is defined as ϕji=θk for every document j and word i. Sometimes, it is easier to use a variable zji that works as an index to sample from the probabilities πjk of Gj (in Gj=∑∞k=1πjkδθk) and then used as in θzji. However, I think this is done in the context of the stick-breaking construction.


[1] http://www.numberjack.net/download/vvb_241_project.pdf
